{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOIqE/ncpvdq200rDtY+s9v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brenda00/postechaiscientist/blob/main/Fase02_Aula01_Pipelines_ETL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Laboratório Prático: Pipeline ETL & Data Quality**\n",
        "\n",
        "\n",
        "Objetivo: Construir um pipeline de dados ponta-a-ponta, transformando dados \"sujos\" em um dataset pronto para treinamento de Machine Learning, utilizando a Arquitetura Medalhão (Bronze, Silver, Gold)."
      ],
      "metadata": {
        "id": "vx-4F7McV-94"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1: Configuração do Ambiente**\n",
        "\n",
        "Primeiro, vamos instalar a biblioteca Faker para gerar dados sintéticos e importar o necessário.\n",
        "\n",
        "Desafio: Você pode consumir alguma API de dado ou até mesmo utilizar um outro arquivo."
      ],
      "metadata": {
        "id": "paOCwQxUWPHD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w82IEzvi8LBx"
      },
      "outputs": [],
      "source": [
        "# Instalação de bibliotecas necessárias\n",
        "!pip install pandas faker pyarrow fastparquet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from faker import Faker\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "\n",
        "# Configuração inicial\n",
        "fake = Faker('pt_BR')\n",
        "Faker.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Criando diretórios para organizar as camadas\n",
        "os.makedirs('data/raw', exist_ok=True)\n",
        "os.makedirs('data/bronze', exist_ok=True)\n",
        "os.makedirs('data/silver', exist_ok=True)\n",
        "os.makedirs('data/gold', exist_ok=True)\n",
        "\n",
        "print(\"Ambiente configurado e pastas criadas!\")"
      ],
      "metadata": {
        "id": "Te-HzqIVUYoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Geração dos Dados (Camada RAW)**\n",
        "\n",
        "Nesta etapa, criaremos um dataset sintético de vendas. Para tornar o cenário realista, vamos inserir propositalmente alguns erros (nulos, datas futuras, valores negativos e duplicatas)."
      ],
      "metadata": {
        "id": "-OjJqUJ1Womc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gerar_dados_raw(num_registros=1000):\n",
        "    dados = []\n",
        "\n",
        "    for _ in range(num_registros):\n",
        "        # Simulando dados com erros ocasionais\n",
        "        record = {\n",
        "            'id_pedido': fake.unique.random_int(min=1, max=10000),\n",
        "            'data_pedido': fake.date_between(start_date='-1y', end_date='+10d'), # +10d gera erro de data futura\n",
        "            'cliente_nome': fake.name() if random.random() > 0.05 else None, # 5% de nulos\n",
        "            'cliente_email': fake.email() if random.random() > 0.05 else \"email_invalido.com\", # 5% de e-mails ruins\n",
        "            'categoria': random.choice(['Eletrônicos', 'Roupas', 'Livros', 'Casa']),\n",
        "            'valor_total': round(random.uniform(-50, 500), 2), # Gera valores negativos propositalmente\n",
        "            'estado_entrega': fake.state_abbr()\n",
        "        }\n",
        "        dados.append(record)\n",
        "\n",
        "    # Criando DataFrame\n",
        "    df = pd.DataFrame(dados)\n",
        "\n",
        "    # Adicionando duplicidade proposital (duplicando 5% dos dados)\n",
        "    df_duplicado = df.sample(frac=0.05)\n",
        "    df_final = pd.concat([df, df_duplicado])\n",
        "\n",
        "    # Salvando como CSV (Simulando o arquivo que chegou do sistema)\n",
        "    caminho_arquivo = 'data/raw/vendas_raw.csv'\n",
        "    df_final.to_csv(caminho_arquivo, index=False)\n",
        "    print(f\"Arquivo RAW gerado em: {caminho_arquivo} com {len(df_final)} registros.\")\n",
        "    return df_final\n",
        "\n",
        "# Executando a geração\n",
        "df_raw = gerar_dados_raw()\n",
        "df_raw.head()"
      ],
      "metadata": {
        "id": "hf0uwgOHXAbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Ingestão na Camada Bronze**\n",
        "\n",
        "Na camada Bronze, o objetivo é apenas ingerir o dado bruto, converter para um formato otimizado (Parquet) e adicionar metadados de rastreabilidade (quando o dado foi processado). Não fazemos limpeza aqui."
      ],
      "metadata": {
        "id": "BJgeCcBRZnKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ingestao_bronze():\n",
        "    # 1. Leitura do dado bruto (RAW)\n",
        "    df = pd.read_csv('data/raw/vendas_raw.csv')\n",
        "\n",
        "    # 2. Adição de Metadados\n",
        "    df['_data_ingestao'] = datetime.now()\n",
        "    df['_origem'] = 'sistema_vendas_csv'\n",
        "\n",
        "    # 3. Salvamento em formato Colunar (Parquet)\n",
        "    caminho_bronze = 'data/bronze/vendas_bronze.parquet'\n",
        "    df.to_parquet(caminho_bronze, index=False)\n",
        "\n",
        "    print(f\"Ingestão Bronze concluída. Arquivo salvo em: {caminho_bronze}\")\n",
        "    return df\n",
        "\n",
        "# Executando ingestão\n",
        "df_bronze = ingestao_bronze()"
      ],
      "metadata": {
        "id": "BvxACrJaXL9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Validação de Qualidade e Camada Silver**\n",
        "\n",
        "Aplicação das regras de qualidade.\n",
        "\n",
        "* Dados válidos vão para a tabela Silver (limpos e deduplicados).\n",
        "* Dados inválidos vão para a Quarentena (para análise posterior)."
      ],
      "metadata": {
        "id": "6Rs6ymPtYgOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def processamento_silver(df):\n",
        "    print(f\"Total de registros recebidos da Bronze: {len(df)}\")\n",
        "\n",
        "    # --- 1. Regras de Qualidade ---\n",
        "\n",
        "    # Regra 1: Campos obrigatórios não podem ser nulos (Nome e ID)\n",
        "    regra_nulos = df['cliente_nome'].notna() & df['id_pedido'].notna()\n",
        "\n",
        "    # Regra 2: Valor total deve ser maior que zero\n",
        "    regra_valor = df['valor_total'] > 0\n",
        "\n",
        "    # Regra 3: Validação simples de e-mail (deve conter '@')\n",
        "    regra_email = df['cliente_email'].str.contains('@', na=False)\n",
        "\n",
        "    # Regra 4: Data do pedido não pode ser futura (maior que hoje)\n",
        "    hoje = datetime.now().strftime('%Y-%m-%d')\n",
        "    regra_data = df['data_pedido'] <= hoje\n",
        "\n",
        "    # Combinando todas as regras (Aprovado se passar em TODAS)\n",
        "    criterio_aprovacao = regra_nulos & regra_valor & regra_email & regra_data\n",
        "\n",
        "    # --- 2. Separação (Split) ---\n",
        "\n",
        "    # Dados Inválidos (Quarentena)\n",
        "    df_quarentena = df[~criterio_aprovacao].copy()\n",
        "\n",
        "    # Dados Válidos (Candidatos a Silver)\n",
        "    df_clean = df[criterio_aprovacao].copy()\n",
        "\n",
        "    # --- 3. Tratamento Final (Limpeza e Padronização) ---\n",
        "\n",
        "    # Remover duplicidades exatas nos dados válidos\n",
        "    df_clean = df_clean.drop_duplicates(subset=['id_pedido'])\n",
        "\n",
        "    # Normalização de strings (ex: Estados para maiúsculo, remover espaços)\n",
        "    df_clean['estado_entrega'] = df_clean['estado_entrega'].str.upper().str.strip()\n",
        "\n",
        "    # --- 4. Salvamento ---\n",
        "    df_clean.to_parquet('data/silver/vendas_silver.parquet', index=False)\n",
        "    df_quarentena.to_csv('data/silver/vendas_quarentena.csv', index=False)\n",
        "\n",
        "    print(\"-\" * 30)\n",
        "    print(f\"Registros enviados para Quarentena (Rejeitados): {len(df_quarentena)}\")\n",
        "    print(f\"Registros aprovados e salvos na Silver: {len(df_clean)}\")\n",
        "\n",
        "    return df_clean, df_quarentena\n",
        "\n",
        "# Executando processamento Silver\n",
        "df_silver, df_quarentena = processamento_silver(df_bronze)"
      ],
      "metadata": {
        "id": "LZV1crR5Yl6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Amostra de dados na Quarentena (com erros):\")\n",
        "display(df_quarentena.head())"
      ],
      "metadata": {
        "id": "A3aARzYGVjCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Camada Gold (Agregações de Negócio)**\n",
        "\n",
        "Agora que temos dados confiáveis na Silver, criamos a camada Gold. Esta camada contém dados prontos para Dashboards e IA. Vamos criar uma visão agregada de vendas por Estado e Categoria."
      ],
      "metadata": {
        "id": "D-CTJj3XYpoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def geracao_gold(df_silver):\n",
        "    # Agregação 1: Receita Total e Ticket Médio por Estado\n",
        "    df_gold_estado = df_silver.groupby('estado_entrega').agg(\n",
        "        total_vendas=('valor_total', 'sum'),\n",
        "        ticket_medio=('valor_total', 'mean'),\n",
        "        qtd_pedidos=('id_pedido', 'count')\n",
        "    ).reset_index().sort_values(by='total_vendas', ascending=False)\n",
        "\n",
        "    # Agregação 2: Performance por Categoria\n",
        "    df_gold_categoria = df_silver.groupby('categoria').agg(\n",
        "        total_vendas=('valor_total', 'sum')\n",
        "    ).reset_index()\n",
        "\n",
        "    # Salvando tabelas Gold\n",
        "    df_gold_estado.to_parquet('data/gold/kpi_vendas_estado.parquet', index=False)\n",
        "    df_gold_categoria.to_parquet('data/gold/kpi_vendas_categoria.parquet', index=False)\n",
        "\n",
        "    print(\"Camada Gold gerada com sucesso!\")\n",
        "    return df_gold_estado\n",
        "\n",
        "# Executando Gold\n",
        "df_gold = geracao_gold(df_silver)"
      ],
      "metadata": {
        "id": "FeFkb2oZYrqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Consumo Final**\n",
        "\n",
        "Para finalizar o laboratório, vamos visualizar o resultado final que seria entregue a um cientista de dados ou analista de BI."
      ],
      "metadata": {
        "id": "q2JnP_E8YvJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulando um analista consumindo o dado pronto\n",
        "print(\"=== Relatório Final de Vendas por Estado (GOLD) ===\")\n",
        "display(df_gold.head(10))\n",
        "\n",
        "# Validação simples\n",
        "print(f\"\\nVerificação: O ticket médio mais alto é de R$ {df_gold['ticket_medio'].max():.2f}\")"
      ],
      "metadata": {
        "id": "gv3ygKZLYxpl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}